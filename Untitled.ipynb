{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](guess.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without thinking about the rules of monopoly it's probably easy to guess as to whether or not a Hotel is under the question mark.\n",
    " \n",
    "Let's suppose you want to predict whether or not a particular house in real life will be expensive or not. If you happen to know that almost all of the houses nearby it are expensive, it would be very reasonable for you to guess that the house is also expensive. This idea of using data that is nearby what you want to predict is central to the Machine Learning method called K-Nearest Neighbors.\n",
    " \n",
    "In K-Nearest Neighbors, you are given training data, for example the location of several houses, and a label for each data point, for example whether that household is expensive or not. You predict the category of any new point, such as a house you don't already know the price of by looking at the K nearest data points, then guessing whichever label the majority of those K data points have.\n",
    " \n",
    "This method is very intuitive and works well when each subset of label's set of data is separated, for example if all expensive houses were in one area and all inexpensive houses were in another area. In most cases though, there will be noise in your data, as in the data points for different labels will mix together like in the figure below. In such a case, looking at multiple points and taking whichever label gains the majority helps defend against being affected by this noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](mixed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how changing the value of K affects K-Nearest-Neighbor by playing with the demo below. As K increases, the regions where you predict a particular label become more and more unusual. You additionally see that increasing K to be arbitrarily large doesn’t necessarily increase the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python demo of KNN region goes here\n"
     ]
    }
   ],
   "source": [
    "print(\"Python demo of KNN region goes here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The importance of Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things that makes a huge difference with how effective this method can be is how you define distance. You want the distance between two data entries to be small if they are similar and large if they are different. The question then becomes, how do you define a distance between two data points that accomplishes this.\n",
    "\n",
    "Each data entry can usually be thought of as a point on a grid and the most common way to define distance between data is to find the distance between their points after you plot them. This can work well in many situations and do very horribly in others. For example, let's say you wanted to try and predict a college student’s sex based on their height (in feet), age and perm number. Clearly the perm number is irrelevant, but it will end up having much more of an effect on the distance, and thus which students are considered more similar. In this circumstance, using the default way to calculate distance would be terrible as the perm number is highly irrelivant.\n",
    "\n",
    "In his paper (Le Cun 1998), Le Cunn attempted to classify images of hand written digits like the ones below.\n",
    "![title](mnist.jpeg)\n",
    "\n",
    "For this problem, we would want the distance between images with the same digit to be small and images with a different digit to be large. Using the default way of defining distance between pictures for this would end up just taking the sum of how different each  pair of pixels are. As in the example in the photo, this can lead to figures which are similarly as bold to be considered similar, while we want figures of a similar shape to be considered similar.\n",
    "![title](prototype.png)\n",
    "\n",
    "In his paper (Le Cun 1998), introduces a new way to calculate distance. The main idea behind it is that rather than comparing the distance between the two points representing the images directly, you consider all the images you can get after applying some transformations such as rotations, translations, stretching, or bolding to either of your original images and find the minimum distance between those. The immediate advantage of this is that images that are literally just rotations or other small deformations away from an image would end up being very close together. The figure below shows different images that have a distance of 0 from the original image.\n",
    "\n",
    "IMAGE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
